{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbGL-l5Wer8c",
        "outputId": "daeeaa90-b0f2-4ab4-cb59-8d05081da728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.0.3\n"
          ]
        }
      ],
      "source": [
        "#1st block\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-3.5.0-bin-hadoop3'\n",
        "!pip install -q findspark\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "LyiwAzC-g8UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2nd Block\n",
        "!ngrok authtoken 2ZDM0s9kUZbJtua2DayJyPs0zCv_5zQvVrspkVxhFqWUdPqkm\n",
        "!ngrok config add-authtoken 2ZDM0s9kUZbJtua2DayJyPs0zCv_5zQvVrspkVxhFqWUdPqkm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_HKGWn4e8Tf",
        "outputId": "60530b0f-f64b-4094-c46f-cc325eabbf17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4th Block\n",
        "!streamlit run streamlit_app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "kHVIhQOCe-Gi"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5th Block\n",
        "!ngrok http 8501 &"
      ],
      "metadata": {
        "id": "Dbib1fa3fGX1"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6th block\n",
        "!ps -eaf | grep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUyfux2jfJmh",
        "outputId": "7eeac64d-1811-4bcf-8324-5bb423570f4a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        4125       1 27 13:08 ?        00:00:01 /usr/bin/python3 /usr/local/bin/streamlit run st\n",
            "root        4156     214  0 13:09 ?        00:00:00 /bin/bash -c ps -eaf | grep streamlit\n",
            "root        4158    4156  0 13:09 ?        00:00:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7th Block\n",
        "!kill -9 4125"
      ],
      "metadata": {
        "id": "W2Jts44qfKav"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3rd Block\n",
        "\n",
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "#init pyspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import os.path\n",
        "import pathlib\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from xgboost.spark import SparkXGBRegressor,SparkXGBClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import GBTClassifier,OneVsRest\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.classification import DecisionTreeClassificationModel, RandomForestClassificationModel, LogisticRegressionModel\n",
        "from pyspark.ml.classification import GBTClassificationModel,OneVsRestModel\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Colab\").config('spark.ui.port', '4050').getOrCreate()\n",
        "\n",
        "st.title(\"Lifepulse Application (Human Activity Recognition)\")\n",
        "\n",
        "selected_model=st.selectbox(\"Which model you want to use?\",(\"Decision Tree Classifier\",\"Random Forest\",\"GBT\", \"Logistic Regression\",\"XGBoost\"))\n",
        "if selected_model:\n",
        "  st.write(\"We will use the {} model to give the Prediction. \".format(selected_model))\n",
        "\n",
        "uploaded_file=st.file_uploader(\"Choose a file containing the sensor data: \")\n",
        "if uploaded_file:\n",
        "  df_pandas=pd.read_csv(uploaded_file)\n",
        "  df = spark.createDataFrame(df_pandas)\n",
        "  window_spec = Window.orderBy(\"timestamp\")\n",
        "  window_size = 10\n",
        "  lagged_cols = [F.lag(col, i).over(window_spec).alias(f\"{col}_lag{i}\") for i in range(window_size,0,-1) for col in [\"timestamp\",\"back_x\", \"back_y\", \"back_z\", \"thigh_x\", \"thigh_y\", \"thigh_z\", \"label\",]]\n",
        "  lagged_df = df.select(*lagged_cols).na.drop()\n",
        "\n",
        "  filtered_df = lagged_df.filter(\n",
        "        (lagged_df.label_lag1 == lagged_df.label_lag2) &\n",
        "        (lagged_df.label_lag2 == lagged_df.label_lag3) &\n",
        "        (lagged_df.label_lag3 == lagged_df.label_lag4) &\n",
        "        (lagged_df.label_lag4 == lagged_df.label_lag5) &\n",
        "        (lagged_df.label_lag5 == lagged_df.label_lag6) &\n",
        "        (lagged_df.label_lag6 == lagged_df.label_lag7) &\n",
        "        (lagged_df.label_lag7 == lagged_df.label_lag8) &\n",
        "        (lagged_df.label_lag8 == lagged_df.label_lag9) &\n",
        "        (lagged_df.label_lag9 == lagged_df.label_lag10)\n",
        "  )\n",
        "  vector_assembler = VectorAssembler(\n",
        "        inputCols=[f\"{col}_lag{i}\" for i in range(window_size, 0, -1) for col in [\"back_x\", \"back_y\", \"back_z\", \"thigh_x\", \"thigh_y\", \"thigh_z\"]],\n",
        "        outputCol=\"features\"\n",
        "  )\n",
        "  lagged_df2 = vector_assembler.transform(filtered_df)\n",
        "  final_df = lagged_df2.select(\"features\", \"label_lag1\").withColumnRenamed(\"label_lag1\", \"label\")\n",
        "  final_df.show(truncate=False)\n",
        "  (training,testing) = final_df.randomSplit([0.3,0.7])\n",
        "  if selected_model==\"Decision Tree Classifier\":\n",
        "    new_dt = DecisionTreeClassificationModel.load('/content/dt/dt')\n",
        "  elif selected_model==\"Random Forest\":\n",
        "    new_dt = RandomForestClassificationModel.load('/content/rf/dt/')\n",
        "  elif selected_model==\"GBT\":\n",
        "    new_dt = OneVsRestModel.load('/content/gbt/gbt/')\n",
        "  elif selected_model==\"Logistic Regression\":\n",
        "    new_dt = LogisticRegressionModel.load('/content/lr/lr/')\n",
        "  elif selected_model==\"XGBoost\":\n",
        "    new_dt = OneVsRestModel.load('/content/xgb/xgb/')\n",
        "  else:\n",
        "    pass\n",
        "  predictions2 = new_dt.transform(testing)\n",
        "  activity_dict={1:'walking', 3: 'shuffling', 4: 'stairs_ascending',5: 'stairs_descending', 6: 'standing', 7: 'sitting',8: 'lying'}\n",
        "  p=predictions2.select(\"prediction\", \"label\").take(5)\n",
        "  pred=[]\n",
        "  actual=[]\n",
        "  for i in range(0,len(p)):\n",
        "    pred.append(activity_dict[p[i][0]])\n",
        "    actual.append(activity_dict[p[i][1]])\n",
        "  df_pred = pd.DataFrame(list(zip(pred, actual)), columns=[\"Prediction\", \"Actual\"])\n",
        "  st.write(df_pred)\n",
        "  evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "  accuracy = evaluator.evaluate(predictions2)\n",
        "  #st.write(\"Test Error = %g \" % (1.0 - accuracy))\n",
        "  st.write(\"Accuracy = %g \" % accuracy)\n",
        "  evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"f1\")\n",
        "  f1 = evaluator.evaluate(predictions2)\n",
        "  st.write(\"F1-Score = %g \" % f1)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRErMUdKfMVa",
        "outputId": "106a6c0d-b29d-4d68-fbf8-3c44d78dfa5a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/xgb.zip -d /content/xgb"
      ],
      "metadata": {
        "id": "5C1-pu7zjaUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf xgb.zip"
      ],
      "metadata": {
        "id": "sXzIkWs6B31O"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}