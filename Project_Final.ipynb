{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install xgboost\n",
        "!pip install findspark"
      ],
      "metadata": {
        "id": "asIIlNmO7wJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmqKDGFh7sO_"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from xgboost.spark import SparkXGBRegressor,SparkXGBClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import GBTClassifier,OneVsRest\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from xgboost.spark import SparkXGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip har70.zip\n",
        "folder_path = \"har70/\"\n",
        "csv_files = os.listdir(folder_path)\n",
        "df = spark.read.csv(os.path.join(folder_path, csv_files[0]), header=True, inferSchema=True)\n",
        "include=['501.csv','502.csv','503.csv']\n",
        "for filename in csv_files[1:]:\n",
        "    if filename in include:\n",
        "        df = df.union(spark.read.csv(os.path.join(folder_path, filename), header=True, inferSchema=True))\n",
        "df.show()\n",
        "print(f\" total rows = {df.count()}\")"
      ],
      "metadata": {
        "id": "if6BWXZT7-Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the window specification to get the rows for each iteration\n",
        "window_spec = Window.orderBy(\"timestamp\")\n",
        "# Define the window size\n",
        "window_size = 10\n",
        "# Create lagged columns using the lag window function\n",
        "lagged_cols = [F.lag(col, i).over(window_spec).alias(f\"{col}_lag{i}\") for i in range(window_size,0,-1) for col in [\"timestamp\",\"back_x\", \"back_y\", \"back_z\", \"thigh_x\", \"thigh_y\", \"thigh_z\", \"label\",]]\n",
        "# Apply the lagged window function and drop rows with null values\n",
        "lagged_df = df.select(*lagged_cols).na.drop()\n",
        "\n",
        "filtered_df = lagged_df.filter(\n",
        "    (lagged_df.label_lag1 == lagged_df.label_lag2) &\n",
        "    (lagged_df.label_lag2 == lagged_df.label_lag3) &\n",
        "    (lagged_df.label_lag3 == lagged_df.label_lag4) &\n",
        "    (lagged_df.label_lag4 == lagged_df.label_lag5) &\n",
        "    (lagged_df.label_lag5 == lagged_df.label_lag6) &\n",
        "    (lagged_df.label_lag6 == lagged_df.label_lag7) &\n",
        "    (lagged_df.label_lag7 == lagged_df.label_lag8) &\n",
        "    (lagged_df.label_lag8 == lagged_df.label_lag9) &\n",
        "    (lagged_df.label_lag9 == lagged_df.label_lag10)\n",
        ")\n",
        "\n",
        "# Show the resu\n",
        "# Create a VectorAssembler to combine the features into a single vector column\n",
        "vector_assembler = VectorAssembler(\n",
        "    inputCols=[f\"{col}_lag{i}\" for i in range(window_size, 0, -1) for col in [\"back_x\", \"back_y\", \"back_z\", \"thigh_x\", \"thigh_y\", \"thigh_z\"]],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "lagged_df2 = vector_assembler.transform(filtered_df)\n",
        "final_df = lagged_df2.select(\"features\", \"label_lag1\").withColumnRenamed(\"label_lag1\", \"label\")\n",
        "final_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "f08Cz7tD8KAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(training,testing) = final_df.randomSplit([0.7,0.3])"
      ],
      "metadata": {
        "id": "dUr4dxht8PYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\",maxDepth= 7)\n",
        "model_dt = dt.fit(training)\n",
        "predictions = model_dt.transform(testing)\n",
        "predictions.select(\"prediction\", \"label\").show(5)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
        "print(\"Accuracy = %g \" % accuracy)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"f1\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"F1-Score = %g \" % accuracy)\n",
        "model_dt.save('dt')\n",
        "new_dt = DecisionTreeClassificationModel.load('dt')"
      ],
      "metadata": {
        "id": "kwdQxCrA8PU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20, maxDepth= 7)\n",
        "model_rf = rf.fit(training)\n",
        "predictions = model_rf.transform(testing)\n",
        "predictions.select(\"prediction\", \"label\").show(5)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
        "print(\"Accuracy = %g \" % accuracy)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"f1\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"F1-Score = %g \" % accuracy)\n",
        "model_rf.save('rf')"
      ],
      "metadata": {
        "id": "V1pJoAGq8PSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",  maxIter=10)\n",
        "ovr = OneVsRest(classifier=gbt, labelCol=\"label\")\n",
        "model_gbt = ovr.fit(training)\n",
        "predictions = model_gbt.transform(testing)\n",
        "predictions.select(\"prediction\", \"label\").show(5)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
        "print(\"Accuracy = %g \" % accuracy)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"f1\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"F1-Score = %g \" % accuracy)\n",
        "model_gbt.save('gbt')"
      ],
      "metadata": {
        "id": "FJ_tIUs78PQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, family=\"multinomial\")\n",
        "model_lr = lr.fit(training)\n",
        "predictions = model_lr.transform(testing)\n",
        "predictions.select(\"prediction\", \"label\").show(5)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
        "print(\"Accuracy = %g \" % accuracy)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"f1\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"F1-Score = %g \" % accuracy)\n",
        "model_lr.save('lr')"
      ],
      "metadata": {
        "id": "HHzCI5dK8POs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = SparkXGBClassifier(\n",
        "  features_col=\"features\",\n",
        "  label_col=\"label\",\n",
        "  num_workers=2,\n",
        ")\n",
        "ovr = OneVsRest(classifier=classifier, labelCol=\"label\")\n",
        "model = ovr.fit(training)\n",
        "predictions = model.transform(testing)\n",
        "predictions.select(\"prediction\", \"label\").show(5)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
        "print(\"Accuracy = %g \" % accuracy)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"f1\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"F1-Score = %g \" % accuracy)\n",
        "model.save('xgb')"
      ],
      "metadata": {
        "id": "1ibH-vjm8PMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using SKlearn ML algorithms"
      ],
      "metadata": {
        "id": "hDa6bzzb9lbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df = final_df.toPandas()"
      ],
      "metadata": {
        "id": "3ngn9vlK9k2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.stack(pandas_df['features'].to_numpy())\n",
        "y = pandas_df['label'].to_numpy()"
      ],
      "metadata": {
        "id": "Ur-8flh293i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier as DCT\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "DWiLJ0g_93e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Wid1dcke97nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = DCT()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(accuracy)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "6HVj4G4L99Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression as LR\n",
        "clf_lr = LR()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf_lr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_lr = clf_lr.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "report_lr = classification_report(y_test, y_pred_lr)\n",
        "\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"Accuracy: {accuracy_lr:.2f}\")\n",
        "print(\"Classification Report:\\n\", report_lr)"
      ],
      "metadata": {
        "id": "YbtJngYD99Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "clf_rf = RFC(n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf_rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_rf = clf_rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "report_rf = classification_report(y_test, y_pred_rf)\n",
        "print(\"\\nRandom Forest:\")\n",
        "print(f\"Accuracy: {accuracy_rf:.2f}\")\n",
        "print(\"Classification Report:\\n\", report_rf)"
      ],
      "metadata": {
        "id": "sLIbHLv5-R-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "clf_gbt = GradientBoostingClassifier(n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf_gbt.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_gbt = clf_gbt.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_gbt = accuracy_score(y_test, y_pred_gbt)\n",
        "report_gbt = classification_report(y_test, y_pred_gbt)\n",
        "\n",
        "print(\"Gradient Boosting Classifier:\")\n",
        "print(f\"Accuracy: {accuracy_gbt:.2f}\")\n",
        "print(\"Classification Report:\\n\", report_gbt)"
      ],
      "metadata": {
        "id": "ANSFyjOo-UJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier as DTC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "clf_dt = DTC()\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'max_depth': [ 3, 5, 7],\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=clf_dt, param_grid=param_grid, cv=3, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Train a new classifier with the best hyperparameters\n",
        "best_clf_dt = DTC(**best_params)\n",
        "best_clf_dt.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_dt = best_clf_dt.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "report_dt = classification_report(y_test, y_pred_dt)\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy_dt))\n",
        "print(\"Classification Report:\\n\", report_dt)\n"
      ],
      "metadata": {
        "id": "6oygAZgA-UEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "clf_rf = RFC()\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 20, 30],\n",
        "    'max_depth':  [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=clf_rf, param_grid=param_grid, cv=3, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params_rf = grid_search.best_params_\n",
        "\n",
        "# Train a new classifier with the best hyperparameters\n",
        "best_clf_rf = RFC(**best_params_rf)\n",
        "best_clf_rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_rf = best_clf_rf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "report_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params_rf)\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy_rf))\n",
        "print(\"Classification Report:\\n\", report_rf)"
      ],
      "metadata": {
        "id": "jSsV_GHw-ZyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X = np.stack(pandas_df['features'].to_numpy())\n",
        "# y = pandas_df['label'].to_numpy()"
      ],
      "metadata": {
        "id": "HHsCiel0-r39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X = X.reshape((X.shape[0], X.shape[1], 1))\n"
      ],
      "metadata": {
        "id": "Dquoafx1-r1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.utils import to_categorical\n",
        "# y_encoded = to_categorical(y)\n"
      ],
      "metadata": {
        "id": "AbYbZd0c-ry-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "7oxz9LLY_KN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential()\n",
        "# model.add(LSTM(units=50, input_shape=(X_train.shape[1], 1)))\n",
        "# model.add(Dense(units=64, activation='relu'))\n",
        "# model.add(Dense(units=9, activation='softmax'))"
      ],
      "metadata": {
        "id": "AZGCvbFI_Nkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LNENmkAp_PzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "NSI02kEo_Prp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}